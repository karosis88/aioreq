{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Aioreq is a Python low-level asynchronous HTTP client library. It is built on top of TCP sockets and implements the HTTP protocol entirely on his own. Install $ pip install aioreq Usage Basic usage >>> import aioreq >>> import asyncio >>> >>> cl = aioreq . Client () >>> >>> resp = asyncio . run ( >>> cl . get ( 'https://www.google.com' ) >>> ) >>> resp < Response 200 OK > >>> resp . status 200 >>> resp . status_message 'OK' >>> resp . request < Request GET https : // www . google . com > >>> headers = resp . headers # dict >>> body = resp . body # bytes object Alternatively, the best practice is to use a Context manager . import aioreq import asyncio async def main (): async with aioreq . Client () as cl : await cl . get ( 'https://google.com' ) More advanced usage This code will asynchronously send 100 get requests to google.com , which is much faster than synchronous libraries. >>> import asyncio >>> import aioreq >>> >>> async def main (): >>> async with aioreq . http . Client () as cl : >>> tasks = [] >>> for j in range ( 100 ): >>> tasks . append ( >>> asyncio . create_task ( >>> cl . get ( 'https://www.google.com/' , ) >>> ) >>> ) >>> await asyncio . gather ( * tasks ) >>> >>> asyncio . run ( main ()) Streams We occasionally use the HTTP protocol to download videos, photos, and possibly files. When downloading very large files, Stream must be used instead of the default Client. When a client downloads videos or files, the server responds with all information including headers, status code, status message, and full body, which can be very large. As a result, we cannot store it in RAM. Stream only returns a portion of the body per iteration, allowing us to write it to disk, then receive another portion and solve the ram overflow problem. There is some fundamental Stream usage. >>> import aioreq >>> import asyncio >>> >>> async def main (): >>> local_file = open ( 'test' , 'wb' ) >>> async with aioreq . StreamClient () as cl : >>> # This code iterates through the message and yields each received chunk separately. >>> async for chunk in cl . get ( 'https://pathtoverybigfile.aioreq' ): >>> local_file . write ( chunk ) Benchmarks Aioreq is a very fast library, and we compared it to other Python libraries to demonstrate its speed. I used these libraries to compare speed. * httpx * requests Benchmark run First, clone aioreq repository . Then... $ cd aioreq $ python -m venv venv $ source ./venv/bin/activate $ pip install '.[benchmarks]' $ cd benchmarks $ python run_tests_functions.py Benchmark results This is the average execution time of each library for 200 asynchronous requests where responses was received without chunked transfer encoding. Benchmark settings. Url - https://www.github.com Requests count - 200 for async and 5 for sync libs With Content-Length $ cd becnhmarks $ python run_tests_functions.py ======================== Benchmark settings Async lib test requests count : 200 Sync lib test requests count : 5 ======================= Function test for aioreq completed. Total time: 1 .2442591340004583 Received statuses { 301 : 200 } Function test for requests completed. Total time: 1 .6835168350007734 Received statuses { 200 : 5 } Function test for httpx completed. Total time: 1 .691718664000291 Received statuses { 301 : 200 } With Transfer-Encoding: Chunked This is the average execution time of each library for 100 asynchronous requests where responses was received with chunked transfer encoding. Benchmark settings. Url - https://www.youtube.com Requests count - 100 for async and 5 for sync libs $ cd benchmarks $ python run_tests_functions.py ======================== Benchmark settings Async lib test requests count : 100 Sync lib test requests count : 5 ======================= Function test for aioreq completed. Total time: 3 .837283965000097 Received statuses { 200 : 100 } Function test for requests completed. Total time: 6 .098562907998712 Received statuses { 200 : 5 } Function test for httpx completed. Total time: 6 .467480723000335 Received statuses { 200 : 100 } As you can see, the synchronous code lags far behind when we make many requests at the same time. Supported Features Aioreq support basic features to work with HTTP/1.1 . More functionality will be avaliable in future realeases. This is the latest version features. * Keep-Alive (Persistent Connections) * Automatic accepting and decoding responses. Using Accept-Encoding header * HTTPS support, TLS/SSL Verification using certifi library * Request Timeouts","title":"Introduction"},{"location":"#install","text":"$ pip install aioreq","title":"Install"},{"location":"#usage","text":"","title":"Usage"},{"location":"#basic-usage","text":">>> import aioreq >>> import asyncio >>> >>> cl = aioreq . Client () >>> >>> resp = asyncio . run ( >>> cl . get ( 'https://www.google.com' ) >>> ) >>> resp < Response 200 OK > >>> resp . status 200 >>> resp . status_message 'OK' >>> resp . request < Request GET https : // www . google . com > >>> headers = resp . headers # dict >>> body = resp . body # bytes object Alternatively, the best practice is to use a Context manager . import aioreq import asyncio async def main (): async with aioreq . Client () as cl : await cl . get ( 'https://google.com' )","title":"Basic usage"},{"location":"#more-advanced-usage","text":"This code will asynchronously send 100 get requests to google.com , which is much faster than synchronous libraries. >>> import asyncio >>> import aioreq >>> >>> async def main (): >>> async with aioreq . http . Client () as cl : >>> tasks = [] >>> for j in range ( 100 ): >>> tasks . append ( >>> asyncio . create_task ( >>> cl . get ( 'https://www.google.com/' , ) >>> ) >>> ) >>> await asyncio . gather ( * tasks ) >>> >>> asyncio . run ( main ())","title":"More advanced usage"},{"location":"#streams","text":"We occasionally use the HTTP protocol to download videos, photos, and possibly files. When downloading very large files, Stream must be used instead of the default Client. When a client downloads videos or files, the server responds with all information including headers, status code, status message, and full body, which can be very large. As a result, we cannot store it in RAM. Stream only returns a portion of the body per iteration, allowing us to write it to disk, then receive another portion and solve the ram overflow problem. There is some fundamental Stream usage. >>> import aioreq >>> import asyncio >>> >>> async def main (): >>> local_file = open ( 'test' , 'wb' ) >>> async with aioreq . StreamClient () as cl : >>> # This code iterates through the message and yields each received chunk separately. >>> async for chunk in cl . get ( 'https://pathtoverybigfile.aioreq' ): >>> local_file . write ( chunk )","title":"Streams"},{"location":"#benchmarks","text":"Aioreq is a very fast library, and we compared it to other Python libraries to demonstrate its speed. I used these libraries to compare speed. * httpx * requests","title":"Benchmarks"},{"location":"#benchmark-run","text":"First, clone aioreq repository . Then...","title":"Benchmark run"},{"location":"#cd-aioreq-python-m-venv-venv-source-venvbinactivate-pip-install-benchmarks-cd-benchmarks-python-run_tests_functionspy","text":"","title":"$ cd aioreq\n$ python -m venv venv\n$ source ./venv/bin/activate\n$ pip install &#39;.[benchmarks]&#39;\n$ cd benchmarks\n$ python run_tests_functions.py\n"},{"location":"#benchmark-results","text":"This is the average execution time of each library for 200 asynchronous requests where responses was received without chunked transfer encoding. Benchmark settings. Url - https://www.github.com Requests count - 200 for async and 5 for sync libs","title":"Benchmark results"},{"location":"#with-content-length","text":"$ cd becnhmarks $ python run_tests_functions.py ======================== Benchmark settings Async lib test requests count : 200 Sync lib test requests count : 5 ======================= Function test for aioreq completed. Total time: 1 .2442591340004583 Received statuses { 301 : 200 } Function test for requests completed. Total time: 1 .6835168350007734 Received statuses { 200 : 5 } Function test for httpx completed. Total time: 1 .691718664000291 Received statuses { 301 : 200 }","title":"With Content-Length"},{"location":"#with-transfer-encoding-chunked","text":"This is the average execution time of each library for 100 asynchronous requests where responses was received with chunked transfer encoding. Benchmark settings. Url - https://www.youtube.com Requests count - 100 for async and 5 for sync libs $ cd benchmarks $ python run_tests_functions.py ======================== Benchmark settings Async lib test requests count : 100 Sync lib test requests count : 5 ======================= Function test for aioreq completed. Total time: 3 .837283965000097 Received statuses { 200 : 100 } Function test for requests completed. Total time: 6 .098562907998712 Received statuses { 200 : 5 } Function test for httpx completed. Total time: 6 .467480723000335 Received statuses { 200 : 100 } As you can see, the synchronous code lags far behind when we make many requests at the same time.","title":"With Transfer-Encoding: Chunked"},{"location":"#supported-features","text":"Aioreq support basic features to work with HTTP/1.1 . More functionality will be avaliable in future realeases. This is the latest version features. * Keep-Alive (Persistent Connections) * Automatic accepting and decoding responses. Using Accept-Encoding header * HTTPS support, TLS/SSL Verification using certifi library * Request Timeouts","title":"Supported Features"},{"location":"advanced/","text":"Headers To work with headers, 'Aioreq' provides special 'BaseHeader' subclasses. We can set the Accept header by simply entering it in the 'Headers' field, as shown here. >>> import aioreq >>> headers = aioreq . Headers ({ 'Accept' : 'application/json' }) We can also prioritize values. >>> headers = aioreq . Headers ({ \"Accept\" : \"application/json; q=1\" }) If you want, you can use the dictionary interface. >>> headers = aioreq . Headers () >>> headers [ 'accept' ] = 'application/json; q=1' However, aioreq's special 'BaseHeader' subclasses can also be used. Why should you use header classes? You can use the editor autocomplete to add headers without knowing anything about header syntax. >>> from aioreq import headers >>> headers = aioreq . Headers () >>> headers . add_header ( headers . Accept ( >>> ( headers . MimeType . json , 1 ), >>> )) >>> headers Headers : accept : application / json ; q = 1 First, we should import headers. >>> from aioreq import headers Then, make a BaseHeader object. >>> header_obj = headers . Accept ( >>> ( headers . MimeType . html , 0.5 ) >>> ) This is the 'Accept' header object, with the value 'application/html; q=0.5,' where 'q' is the priority for that 'MimeType.' Finally, insert this header into the 'Headers' object. >>> headers . add_header ( header_obj ) >>> headers Headers : accept : application / html ; q = 0.5 We can also add multiple 'Values' in this manner. >>> header_obj = Accept ( >>> ( headers . MimeType . html , 0.6 ), >>> ( headers . MimeType . json , 0.8 ) >>> ) >>> headers . add_header ( header_obj ) >>> headers : Headers : accept : application / html ; q = 0.6 , application / json ; q = 0.8 That is, the client accepts both html and json responses, but please provide me with a json response if you support both. Client The Client class offers a high-level API for sending and receiving HTTP/1.1 requests. That is how the 'Client' class works. >>> import aioreq >>> async def main (): >>> async with aioreq . Client () as client : >>> ... To change some default arguments or enable persistent connections, pass arguments through the 'Client.' >>> async def main (): >>> async with aioreq . Client ( headers = { 'test' : 'test1' }) as client : >>> await client . get ( 'http://example.com' ) # This request includes the 'test' header, which was added by the client. >>> await client . get ( 'http://example.com' , headers = { 'test' : 'test2' }) # Overrides the client headers. You can also use HTTP/1.1's main feature, persistent connections, to save a significant amount of memory and system resources by reusing connections rather than creating new ones. >>> async def main (): >>> async with aioreq . Client ( persistent_connections = True ) as client : >>> ... # This client can now reuse previously opened connections. In order to improve request performance, 'Aioreq' instructs the server to encode HTTP messages if possible. This feature is very useful, but you can disable it for specific 'Client' objects if you want. >>> client = aioreq . Client ( enable_encodings = False )","title":"Advanced Usage"},{"location":"advanced/#headers","text":"To work with headers, 'Aioreq' provides special 'BaseHeader' subclasses. We can set the Accept header by simply entering it in the 'Headers' field, as shown here. >>> import aioreq >>> headers = aioreq . Headers ({ 'Accept' : 'application/json' }) We can also prioritize values. >>> headers = aioreq . Headers ({ \"Accept\" : \"application/json; q=1\" }) If you want, you can use the dictionary interface. >>> headers = aioreq . Headers () >>> headers [ 'accept' ] = 'application/json; q=1' However, aioreq's special 'BaseHeader' subclasses can also be used. Why should you use header classes? You can use the editor autocomplete to add headers without knowing anything about header syntax. >>> from aioreq import headers >>> headers = aioreq . Headers () >>> headers . add_header ( headers . Accept ( >>> ( headers . MimeType . json , 1 ), >>> )) >>> headers Headers : accept : application / json ; q = 1 First, we should import headers. >>> from aioreq import headers Then, make a BaseHeader object. >>> header_obj = headers . Accept ( >>> ( headers . MimeType . html , 0.5 ) >>> ) This is the 'Accept' header object, with the value 'application/html; q=0.5,' where 'q' is the priority for that 'MimeType.' Finally, insert this header into the 'Headers' object. >>> headers . add_header ( header_obj ) >>> headers Headers : accept : application / html ; q = 0.5 We can also add multiple 'Values' in this manner. >>> header_obj = Accept ( >>> ( headers . MimeType . html , 0.6 ), >>> ( headers . MimeType . json , 0.8 ) >>> ) >>> headers . add_header ( header_obj ) >>> headers : Headers : accept : application / html ; q = 0.6 , application / json ; q = 0.8 That is, the client accepts both html and json responses, but please provide me with a json response if you support both.","title":"Headers"},{"location":"advanced/#client","text":"The Client class offers a high-level API for sending and receiving HTTP/1.1 requests. That is how the 'Client' class works. >>> import aioreq >>> async def main (): >>> async with aioreq . Client () as client : >>> ... To change some default arguments or enable persistent connections, pass arguments through the 'Client.' >>> async def main (): >>> async with aioreq . Client ( headers = { 'test' : 'test1' }) as client : >>> await client . get ( 'http://example.com' ) # This request includes the 'test' header, which was added by the client. >>> await client . get ( 'http://example.com' , headers = { 'test' : 'test2' }) # Overrides the client headers. You can also use HTTP/1.1's main feature, persistent connections, to save a significant amount of memory and system resources by reusing connections rather than creating new ones. >>> async def main (): >>> async with aioreq . Client ( persistent_connections = True ) as client : >>> ... # This client can now reuse previously opened connections. In order to improve request performance, 'Aioreq' instructs the server to encode HTTP messages if possible. This feature is very useful, but you can disable it for specific 'Client' objects if you want. >>> client = aioreq . Client ( enable_encodings = False )","title":"Client"},{"location":"quickstart/","text":"Usage Basic usage First, import aioreq. >>> import aioreq This is a very basic example of usage. >>> client = aioreq . Client () >>> import asyncio >>> aynsio . run ( client . get ( 'http://google.com' )) < Response 200 b 'OK' > The client's context manager stores all of his connections and files and handles cleanup when we're done with them. This is a suggested setup. >>> async def main (): >>> async with aioreq . Client () as client : >>> response = client . get ( \"http://google.com\" ) This is how 'response' objects are used. >>> response . request < Request GET http : // googlge . com > >>> response . status 200 >>> response . status_message b 'OK' Aioreq provides complete header control. >>> response . headers Headers : date : Thu , 24 Nov 2022 05 : 50 : 34 GMT expires : - 1 cache - control : private , max - age = 0 content - type : text / html ; charset = ISO - 8859 - 1 cross - origin - opener - policy - report - only : same - origin - allow - popups ; report - to = \"gws\" report - to : { \"group\" : \"gws\" , \"max_age\" : 2592000 , \"endpoints\" :[{ \"url\" : \"https://csp.withgoogle.com/csp/report-to/gws/other\" }]} p3p : CP = \"This is not a P3P policy! See g.co/p3phelp for more info.\" content - encoding : gzip server : gws content - length : 16622 x - xss - protection : 0 x - frame - options : SAMEORIGIN As you can see, there is a 'content-encoding' header, and it is gzip. Aioreq can automatically handle many types of encodings and can easily receive compressed data instead of raw data. This is how accessing the body message works. >>> body = response . content >>> type ( body ) bytes >>> len ( body ) 50021 As you can see, the 'content-length' header has the value '16622,' indicating that the incoming data length should be 16622, but our content field contains significantly more data than expected. It's because 'Aioreq' 'automatically told the server' to use compression for better performance, which is then decoded on the client. If you need to receive very lots of data, use 'StreamClient' instead of 'Client.' 'StreamClient' methods return 'async iterators,' which allow us to receive only a small amount of data per iteration and write it to the hard drive, saving a lot of memory. >>> async def main (): >>> file = open ( 'index.html' , 'wb' ) >>> async with aioreq . StreamClient () as stream_client : >>> async for chunk in stream_client . get ( \"http://aioreq.example.com\" ) >>> file . write ( chunk ) >>> file . close () Headers This is how Headers are set up. >>> import aioreq >>> headers = aioreq . Headers () 'Aioreq Headers' are objects that are similar to dictionary objects but have some differences. case-insensitivity >>> headers [ 'my-header' ] = 'my-text' >>> headers [ 'MY-HeAdEr' ] 'my-text' pretty-print >>> headers Headers : my - header : my - text aioreq header types compatibility >>> from aioreq import headers >>> new_header = aioreq . Headers () >>> header_obj = headers . Accept ( types = ( ( headers . MimeType . json , 0.5 ), )) >>> new_header . add_header ( header_obj ) >>> new_header Headers : accept : application / json ; q = 0.5 Requests There is how to make simple GET request. >>> import aioreq >>> async def main (): >>> async with aioreq . Client () as client : >>> await client . get ( \"https://google.com\" ) Alternatively, we can create a Request object and send it directly through the client. >>> req = aioreq . Request ( ... url = 'https://google.com/' , ... method = 'GET' , ... ) >>> await cl . send_request ( req ) What about sending path parameters? >>> req = aioreq . Request ( ... url = 'https://google.com/' , ... method = 'GET' , ... params = (( 'example_1' , 10 ), ( 'example_2' , 20 )) ... ) and perhaps a body message >>> req = aioreq . Request ( ... url = 'https://google.com' , ... method = 'GET' , ... content = b 'Text for the body' ) If we want to send a JSON request, we must include the content-type parameter. >>> req = aioerq . Request ( ... url = 'https://google.com' , ... method = 'GET' , ... content = b '{\"test\": \"test\"}' , ... headers = { 'content-type' : 'application/json' }) Alternatively, we can use JsonRequest. >>> req = aioerq . JsonRequest ( ... url = 'https://google.com' , ... method = 'GET' , ... content = b '{\"test\": \"test\"}' , ... ) Each response object contains his request. The 'request' field provides access to the Request object. >>> response . request < Request GET https : // google . com > Clients Initialization of the client. >>> client = aioreq . Client () >>> stream_client = aioerq . StreamClient () or >>> async with aioreq . Client () as client , aioreq . StreamClient () as stream_client : ... ... You can provide the client with default headers. The client employs his headers in all of his requests. >>> aioreq . Client ( headers = { 'Accept' : 'application/json' }) The initialization interface for StreamClient is the same, but the request sending logic is different. This is how StreamClient requests works. >>> async def main ( file ): >>> async with aioreq . StreamClient () as stream_client : >>> async for chunk in stream_client . get ( 'https://youtube.com' ): >>> file . write ( chunk ) You can use StreamClient if you don't need a full response right away and want to save a lot of RAM.","title":"Usage"},{"location":"quickstart/#usage","text":"","title":"Usage"},{"location":"quickstart/#basic-usage","text":"First, import aioreq. >>> import aioreq This is a very basic example of usage. >>> client = aioreq . Client () >>> import asyncio >>> aynsio . run ( client . get ( 'http://google.com' )) < Response 200 b 'OK' > The client's context manager stores all of his connections and files and handles cleanup when we're done with them. This is a suggested setup. >>> async def main (): >>> async with aioreq . Client () as client : >>> response = client . get ( \"http://google.com\" ) This is how 'response' objects are used. >>> response . request < Request GET http : // googlge . com > >>> response . status 200 >>> response . status_message b 'OK' Aioreq provides complete header control. >>> response . headers Headers : date : Thu , 24 Nov 2022 05 : 50 : 34 GMT expires : - 1 cache - control : private , max - age = 0 content - type : text / html ; charset = ISO - 8859 - 1 cross - origin - opener - policy - report - only : same - origin - allow - popups ; report - to = \"gws\" report - to : { \"group\" : \"gws\" , \"max_age\" : 2592000 , \"endpoints\" :[{ \"url\" : \"https://csp.withgoogle.com/csp/report-to/gws/other\" }]} p3p : CP = \"This is not a P3P policy! See g.co/p3phelp for more info.\" content - encoding : gzip server : gws content - length : 16622 x - xss - protection : 0 x - frame - options : SAMEORIGIN As you can see, there is a 'content-encoding' header, and it is gzip. Aioreq can automatically handle many types of encodings and can easily receive compressed data instead of raw data. This is how accessing the body message works. >>> body = response . content >>> type ( body ) bytes >>> len ( body ) 50021 As you can see, the 'content-length' header has the value '16622,' indicating that the incoming data length should be 16622, but our content field contains significantly more data than expected. It's because 'Aioreq' 'automatically told the server' to use compression for better performance, which is then decoded on the client. If you need to receive very lots of data, use 'StreamClient' instead of 'Client.' 'StreamClient' methods return 'async iterators,' which allow us to receive only a small amount of data per iteration and write it to the hard drive, saving a lot of memory. >>> async def main (): >>> file = open ( 'index.html' , 'wb' ) >>> async with aioreq . StreamClient () as stream_client : >>> async for chunk in stream_client . get ( \"http://aioreq.example.com\" ) >>> file . write ( chunk ) >>> file . close ()","title":"Basic usage"},{"location":"quickstart/#headers","text":"This is how Headers are set up. >>> import aioreq >>> headers = aioreq . Headers () 'Aioreq Headers' are objects that are similar to dictionary objects but have some differences. case-insensitivity >>> headers [ 'my-header' ] = 'my-text' >>> headers [ 'MY-HeAdEr' ] 'my-text' pretty-print >>> headers Headers : my - header : my - text aioreq header types compatibility >>> from aioreq import headers >>> new_header = aioreq . Headers () >>> header_obj = headers . Accept ( types = ( ( headers . MimeType . json , 0.5 ), )) >>> new_header . add_header ( header_obj ) >>> new_header Headers : accept : application / json ; q = 0.5","title":"Headers"},{"location":"quickstart/#requests","text":"There is how to make simple GET request. >>> import aioreq >>> async def main (): >>> async with aioreq . Client () as client : >>> await client . get ( \"https://google.com\" ) Alternatively, we can create a Request object and send it directly through the client. >>> req = aioreq . Request ( ... url = 'https://google.com/' , ... method = 'GET' , ... ) >>> await cl . send_request ( req ) What about sending path parameters? >>> req = aioreq . Request ( ... url = 'https://google.com/' , ... method = 'GET' , ... params = (( 'example_1' , 10 ), ( 'example_2' , 20 )) ... ) and perhaps a body message >>> req = aioreq . Request ( ... url = 'https://google.com' , ... method = 'GET' , ... content = b 'Text for the body' ) If we want to send a JSON request, we must include the content-type parameter. >>> req = aioerq . Request ( ... url = 'https://google.com' , ... method = 'GET' , ... content = b '{\"test\": \"test\"}' , ... headers = { 'content-type' : 'application/json' }) Alternatively, we can use JsonRequest. >>> req = aioerq . JsonRequest ( ... url = 'https://google.com' , ... method = 'GET' , ... content = b '{\"test\": \"test\"}' , ... ) Each response object contains his request. The 'request' field provides access to the Request object. >>> response . request < Request GET https : // google . com >","title":"Requests"},{"location":"quickstart/#clients","text":"Initialization of the client. >>> client = aioreq . Client () >>> stream_client = aioerq . StreamClient () or >>> async with aioreq . Client () as client , aioreq . StreamClient () as stream_client : ... ... You can provide the client with default headers. The client employs his headers in all of his requests. >>> aioreq . Client ( headers = { 'Accept' : 'application/json' }) The initialization interface for StreamClient is the same, but the request sending logic is different. This is how StreamClient requests works. >>> async def main ( file ): >>> async with aioreq . StreamClient () as stream_client : >>> async for chunk in stream_client . get ( 'https://youtube.com' ): >>> file . write ( chunk ) You can use StreamClient if you don't need a full response right away and want to save a lot of RAM.","title":"Clients"}]}